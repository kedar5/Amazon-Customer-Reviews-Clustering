{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd240a1b",
   "metadata": {},
   "source": [
    "## AMAZON REVIEW CLUSTERING - ksdeshpa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab04bf",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8174adec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['overall' 'vote' 'verified' 'reviewTime' 'reviewerID' 'asin'\n",
      " 'reviewerName' 'reviewText' 'summary' 'unixReviewTime' 'style' 'image']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "true = True\n",
    "\n",
    "false = False\n",
    "def parse(path):\n",
    "    g=gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDataFrame(path):\n",
    "    i=0\n",
    "    df ={}\n",
    "    data = parse(path)\n",
    "    for d in data:\n",
    "        df[i] =d\n",
    "        i += 1\n",
    "        if i > 100000:\n",
    "            break\n",
    "    return pd.DataFrame.from_dict(df, orient ='index')\n",
    "\n",
    "df = getDataFrame(\"C:/Users/Kedar/Desktop/AML/Amazon/Home_and_Kitchen.json.gz\")\n",
    "\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4715782d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>08 31, 2010</td>\n",
       "      <td>A3NSN9WOX8470M</td>\n",
       "      <td>0006564224</td>\n",
       "      <td>mmm</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "      <td>Many uses...</td>\n",
       "      <td>1283212800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>04 2, 2010</td>\n",
       "      <td>A2AMX0AJ2BUDNV</td>\n",
       "      <td>0006564224</td>\n",
       "      <td>John R. Welch</td>\n",
       "      <td>Seems a bit expensive for a plastic bottle, bu...</td>\n",
       "      <td>Dispenser bottle</td>\n",
       "      <td>1270166400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>11 5, 2015</td>\n",
       "      <td>A8LUWTIPU9CZB</td>\n",
       "      <td>0560467893</td>\n",
       "      <td>Linda Fahner</td>\n",
       "      <td>Great product, love it!!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1446681600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>10 29, 2015</td>\n",
       "      <td>AABKIIHAL0L66</td>\n",
       "      <td>0560467893</td>\n",
       "      <td>TheBlueChain</td>\n",
       "      <td>This is a sturdy floating corner shelf!  We mo...</td>\n",
       "      <td>Sturdy Shelf, Poor Installation Instructions</td>\n",
       "      <td>1446076800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>09 9, 2015</td>\n",
       "      <td>A3DA0KIQ5OBK5C</td>\n",
       "      <td>0560467893</td>\n",
       "      <td>angelaarden</td>\n",
       "      <td>I purchased 4 of these shelves. they look grea...</td>\n",
       "      <td>Look great - one bad one...</td>\n",
       "      <td>1441756800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall vote  verified   reviewTime      reviewerID        asin  \\\n",
       "0      5.0    2      True  08 31, 2010  A3NSN9WOX8470M  0006564224   \n",
       "1      5.0    2      True   04 2, 2010  A2AMX0AJ2BUDNV  0006564224   \n",
       "2      5.0  NaN      True   11 5, 2015   A8LUWTIPU9CZB  0560467893   \n",
       "3      4.0    4      True  10 29, 2015   AABKIIHAL0L66  0560467893   \n",
       "4      3.0  NaN      True   09 9, 2015  A3DA0KIQ5OBK5C  0560467893   \n",
       "\n",
       "    reviewerName                                         reviewText  \\\n",
       "0            mmm  I don't use these for their original use, and ...   \n",
       "1  John R. Welch  Seems a bit expensive for a plastic bottle, bu...   \n",
       "2   Linda Fahner                           Great product, love it!!   \n",
       "3   TheBlueChain  This is a sturdy floating corner shelf!  We mo...   \n",
       "4    angelaarden  I purchased 4 of these shelves. they look grea...   \n",
       "\n",
       "                                        summary  unixReviewTime style image  \n",
       "0                                  Many uses...      1283212800   NaN   NaN  \n",
       "1                              Dispenser bottle      1270166400   NaN   NaN  \n",
       "2                                    Five Stars      1446681600   NaN   NaN  \n",
       "3  Sturdy Shelf, Poor Installation Instructions      1446076800   NaN   NaN  \n",
       "4                   Look great - one bad one...      1441756800   NaN   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "corpus = df['reviewText'].values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566f1b4",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28996d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string,re\n",
    "def clean_text(text): \n",
    "\n",
    "    # remove punctuation\n",
    "    text_cleaned = \"\".join([x for x in text if x not in string.punctuation]) \n",
    "        \n",
    "    # Remove extra white space \n",
    "    text_cleaned = re.sub(' +', ' ', text_cleaned)\n",
    "    text_cleaned = text_cleaned.lower()\n",
    "    \n",
    "    tokens = text_cleaned.split(\" \")  \n",
    "\n",
    "    return text_cleaned\n",
    "corpus = df['reviewText'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59c5809d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image             0.008090\n",
       "vote              0.178718\n",
       "style             0.741553\n",
       "summary           0.999870\n",
       "reviewerName      0.999940\n",
       "overall           1.000000\n",
       "verified          1.000000\n",
       "reviewTime        1.000000\n",
       "reviewerID        1.000000\n",
       "asin              1.000000\n",
       "reviewText        1.000000\n",
       "unixReviewTime    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.notnull().mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7ccfbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "# The WordNet Lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Acceptable parts of speech\n",
    "pos_map = {'N': wordnet.NOUN, 'J': wordnet.ADJ, 'V': wordnet.VERB, 'R': wordnet.ADV }\n",
    "# Function to reduce corpus to word lemmas, limited to nouns, verbs, etc. \n",
    "def clean_corpus(corpus):\n",
    "    cleaned = []\n",
    "    for text in corpus:\n",
    "        words = []\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            for (word, pos) in nltk.pos_tag(nltk.word_tokenize(sent)):\n",
    "                if not pos[0] in pos_map:\n",
    "                    continue\n",
    "                word = lemmatizer.lemmatize(word.lower(), pos_map[pos[0]])\n",
    "                words.append(word.lower())\n",
    "        cleaned.append(words)\n",
    "    return cleaned\n",
    "clean_texts = clean_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bac3b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be             179495\n",
      "i              158518\n",
      "have           75142\n",
      "use            48124\n",
      "not            33880\n",
      "make           30360\n",
      "great          28479\n",
      "do             28027\n",
      "work           24953\n",
      "very           24748\n",
      "good           24168\n",
      "get            23710\n",
      "so             22183\n",
      "well           20364\n",
      "buy            20281\n",
      "just           19380\n",
      "love           18238\n",
      "time           18230\n",
      "easy           18047\n",
      "product        15532\n",
      "up             14837\n",
      "year           14504\n",
      "more           12354\n",
      "clean          11979\n",
      "out            11322\n",
      "only           11244\n",
      "go             10339\n",
      "much           10267\n",
      "little         10183\n",
      "need           10014\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Print the 30 most frequent words, and add to stopword list\n",
    "counter = Counter([w for t in clean_texts for w in t])\n",
    "wordcounts = counter.most_common(n=30)\n",
    "stopwords = []\n",
    "for word, count in wordcounts:\n",
    "    stopwords.append(word)\n",
    "    print('%s%i' % (word.ljust(15), count))\n",
    "    \n",
    "stopwords.extend(['from','nice', 'like', 'Like',\\\n",
    "    'stand', 'great', 'well','good','product','put',\\\n",
    "    'buy', 'ago', 're', 'edu', 'use', 'from', 'my', 'we',\\\n",
    "    'i', 've', 'buy', 'set', 'lot', 'decide', 'give', 'add'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13c42d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from each lemmatized text block\n",
    "for i in range(len(clean_texts)):\n",
    "    j = 0\n",
    "    while j < len(clean_texts[i]):\n",
    "        if clean_texts[i][j] in stopwords:\n",
    "            del clean_texts[i][j]\n",
    "        else:\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7829674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dont', 'original', 'purchase', 'replacement', 'last', 'never', 'even', 'think', 'internet', 'find', 'new', 'one', 'pleasant', 'surprise', 'find', 'immediately', 'actually', 'pay', 'less']\n"
     ]
    }
   ],
   "source": [
    "# Show example text block after data preparation\n",
    "print(clean_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd39ac",
   "metadata": {},
   "source": [
    "## Creating corpus and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72ebbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "dictionary = Dictionary(clean_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in clean_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a7531f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# Pretrain the model using the corpus and number of topics\n",
    "num_topics = 20\n",
    "lda = LdaMulticore(corpus, id2word=dictionary, num_topics=num_topics, passes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0135c",
   "metadata": {},
   "source": [
    "## Get Coherence Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f9fced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4958895403220406\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=clean_texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d4300",
   "metadata": {},
   "source": [
    "## Topic Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebe4323b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.153*\"pan\" + 0.053*\"whisk\" + 0.026*\"nonstick\" + 0.014*\"spoon\" + 0.012*\"scratch\" + 0.011*\"toast\" + 0.010*\"ok\" + 0.009*\"stick\" + 0.008*\"perfectly\" + 0.007*\"silicone\" + 0.007*\"egg\" + 0.006*\"surface\" + 0.006*\"liner\" + 0.006*\"spray\" + 0.006*\"par\" + 0.006*\"look\" + 0.005*\"utensil\" + 0.005*\"inch\" + 0.005*\"other\" + 0.005*\"charm\"\n",
      "\n",
      "Topic 1: 0.032*\"tea\" + 0.029*\"water\" + 0.017*\"hot\" + 0.015*\"kettle\" + 0.012*\"keep\" + 0.008*\"heat\" + 0.008*\"leave\" + 0.008*\"ive\" + 0.008*\"dont\" + 0.008*\"lid\" + 0.008*\"top\" + 0.007*\"still\" + 0.006*\"boil\" + 0.006*\"off\" + 0.006*\"stove\" + 0.005*\"cupcake\" + 0.005*\"dry\" + 0.005*\"doesnt\" + 0.005*\"take\" + 0.005*\"long\"\n",
      "\n",
      "Topic 2: 0.023*\"blade\" + 0.017*\"handle\" + 0.014*\"cut\" + 0.014*\"bowl\" + 0.011*\"edge\" + 0.011*\"hand\" + 0.010*\"sharp\" + 0.009*\"knife\" + 0.009*\"hold\" + 0.009*\"too\" + 0.008*\"potato\" + 0.008*\"slice\" + 0.008*\"really\" + 0.007*\"sharpen\" + 0.007*\"small\" + 0.007*\"dont\" + 0.007*\"apple\" + 0.006*\"juice\" + 0.006*\"also\" + 0.006*\"kitchen\"\n",
      "\n",
      "Topic 3: 0.037*\"pressure\" + 0.033*\"cooker\" + 0.028*\"rice\" + 0.026*\"cook\" + 0.025*\"canner\" + 0.014*\"pot\" + 0.007*\"can\" + 0.007*\"seal\" + 0.007*\"lid\" + 0.006*\"steam\" + 0.006*\"food\" + 0.006*\"quart\" + 0.006*\"meat\" + 0.006*\"first\" + 0.006*\"minute\" + 0.006*\"gasket\" + 0.006*\"chicken\" + 0.006*\"water\" + 0.005*\"thing\" + 0.005*\"come\"\n",
      "\n",
      "Topic 4: 0.021*\"oven\" + 0.014*\"popper\" + 0.013*\"waffle\" + 0.012*\"toaster\" + 0.011*\"lid\" + 0.010*\"oil\" + 0.009*\"popcorn\" + 0.008*\"grill\" + 0.008*\"iron\" + 0.007*\"jar\" + 0.006*\"dont\" + 0.006*\"fit\" + 0.006*\"heavy\" + 0.005*\"side\" + 0.005*\"other\" + 0.005*\"as\" + 0.005*\"even\" + 0.005*\"pan\" + 0.005*\"open\" + 0.005*\"food\"\n",
      "\n",
      "Topic 5: 0.022*\"timer\" + 0.013*\"loud\" + 0.010*\"thing\" + 0.010*\"dont\" + 0.009*\"cant\" + 0.008*\"hear\" + 0.007*\"italy\" + 0.007*\"back\" + 0.006*\"neck\" + 0.005*\"dog\" + 0.005*\"foot\" + 0.005*\"expresso\" + 0.005*\"say\" + 0.005*\"room\" + 0.005*\"blender\" + 0.005*\"know\" + 0.005*\"fan\" + 0.005*\"house\" + 0.005*\"war\" + 0.005*\"look\"\n",
      "\n",
      "Topic 6: 0.050*\"disc\" + 0.026*\"battery\" + 0.021*\"slicer\" + 0.020*\"baking\" + 0.019*\"fun\" + 0.012*\"cooky\" + 0.011*\"cookie\" + 0.011*\"mom\" + 0.008*\"mold\" + 0.006*\"kid\" + 0.006*\"caddy\" + 0.006*\"cuisinart\" + 0.006*\"mite\" + 0.006*\"life\" + 0.006*\"cute\" + 0.005*\"describe\" + 0.005*\"pocket\" + 0.005*\"mat\" + 0.005*\"yet\" + 0.004*\"word\"\n",
      "\n",
      "Topic 7: 0.044*\"quality\" + 0.040*\"recommend\" + 0.025*\"price\" + 0.023*\"highly\" + 0.020*\"size\" + 0.016*\"look\" + 0.015*\"excellent\" + 0.012*\"sturdy\" + 0.011*\"kitchen\" + 0.011*\"high\" + 0.009*\"small\" + 0.009*\"cutter\" + 0.009*\"perfect\" + 0.008*\"anyone\" + 0.008*\"fit\" + 0.008*\"large\" + 0.008*\"pizza\" + 0.008*\"hand\" + 0.007*\"purchase\" + 0.007*\"best\"\n",
      "\n",
      "Topic 8: 0.013*\"come\" + 0.012*\"fast\" + 0.011*\"find\" + 0.011*\"ive\" + 0.009*\"look\" + 0.009*\"simple\" + 0.009*\"dont\" + 0.009*\"last\" + 0.008*\"cook\" + 0.008*\"aluminum\" + 0.007*\"worth\" + 0.007*\"instruction\" + 0.007*\"american\" + 0.007*\"shipping\" + 0.007*\"quick\" + 0.006*\"now\" + 0.006*\"expensive\" + 0.006*\"want\" + 0.006*\"read\" + 0.006*\"price\"\n",
      "\n",
      "Topic 9: 0.033*\"unit\" + 0.014*\"keep\" + 0.012*\"holder\" + 0.012*\"first\" + 0.011*\"motor\" + 0.010*\"button\" + 0.009*\"timer\" + 0.009*\"second\" + 0.007*\"suction\" + 0.006*\"power\" + 0.006*\"off\" + 0.006*\"few\" + 0.006*\"down\" + 0.005*\"too\" + 0.005*\"alarm\" + 0.005*\"attachment\" + 0.005*\"powerful\" + 0.005*\"switch\" + 0.005*\"month\" + 0.005*\"problem\"\n",
      "\n",
      "Topic 10: 0.026*\"old\" + 0.019*\"espresso\" + 0.011*\"replace\" + 0.010*\"model\" + 0.010*\"press\" + 0.009*\"one\" + 0.009*\"braun\" + 0.009*\"break\" + 0.008*\"new\" + 0.008*\"thermos\" + 0.008*\"plastic\" + 0.008*\"handle\" + 0.008*\"screw\" + 0.008*\"as\" + 0.007*\"last\" + 0.007*\"take\" + 0.006*\"other\" + 0.006*\"electric\" + 0.006*\"fine\" + 0.006*\"purchase\"\n",
      "\n",
      "Topic 11: 0.076*\"knife\" + 0.033*\"cake\" + 0.025*\"gift\" + 0.019*\"purchase\" + 0.015*\"sharp\" + 0.013*\"really\" + 0.013*\"bake\" + 0.012*\"christmas\" + 0.011*\"nutmeg\" + 0.010*\"daughter\" + 0.010*\"expect\" + 0.009*\"come\" + 0.008*\"cut\" + 0.008*\"perfect\" + 0.007*\"receive\" + 0.007*\"kitchen\" + 0.007*\"happy\" + 0.006*\"friend\" + 0.006*\"cookie\" + 0.006*\"wonderful\"\n",
      "\n",
      "Topic 12: 0.035*\"blender\" + 0.020*\"processor\" + 0.014*\"small\" + 0.013*\"food\" + 0.010*\"drink\" + 0.010*\"want\" + 0.010*\"think\" + 0.010*\"cuisinart\" + 0.009*\"garlic\" + 0.009*\"plastic\" + 0.008*\"counter\" + 0.008*\"ice\" + 0.008*\"dont\" + 0.007*\"press\" + 0.007*\"big\" + 0.007*\"blend\" + 0.006*\"bread\" + 0.006*\"look\" + 0.006*\"blade\" + 0.006*\"french\"\n",
      "\n",
      "Topic 13: 0.031*\"bread\" + 0.017*\"item\" + 0.016*\"salad\" + 0.016*\"return\" + 0.015*\"machine\" + 0.014*\"spinner\" + 0.013*\"loaf\" + 0.011*\"review\" + 0.011*\"store\" + 0.010*\"amazon\" + 0.010*\"purchase\" + 0.009*\"price\" + 0.008*\"now\" + 0.008*\"find\" + 0.008*\"didnt\" + 0.008*\"money\" + 0.008*\"try\" + 0.007*\"im\" + 0.006*\"first\" + 0.006*\"back\"\n",
      "\n",
      "Topic 14: 0.017*\"blender\" + 0.012*\"new\" + 0.010*\"order\" + 0.009*\"break\" + 0.008*\"replacement\" + 0.008*\"blade\" + 0.008*\"now\" + 0.007*\"problem\" + 0.007*\"month\" + 0.007*\"part\" + 0.007*\"jar\" + 0.007*\"can\" + 0.007*\"look\" + 0.007*\"amazon\" + 0.007*\"replace\" + 0.007*\"same\" + 0.006*\"purchase\" + 0.006*\"send\" + 0.006*\"last\" + 0.006*\"service\"\n",
      "\n",
      "Topic 15: 0.021*\"exactly\" + 0.018*\"awesome\" + 0.018*\"thanks\" + 0.016*\"arrive\" + 0.013*\"want\" + 0.013*\"item\" + 0.011*\"popcorn\" + 0.011*\"ship\" + 0.010*\"infuser\" + 0.008*\"fast\" + 0.008*\"described\" + 0.008*\"advertised\" + 0.008*\"price\" + 0.007*\"butter\" + 0.007*\"quickly\" + 0.007*\"hose\" + 0.007*\"amazon\" + 0.006*\"salt\" + 0.006*\"expect\" + 0.006*\"pop\"\n",
      "\n",
      "Topic 16: 0.081*\"coffee\" + 0.039*\"cup\" + 0.018*\"maker\" + 0.013*\"grind\" + 0.013*\"pot\" + 0.012*\"machine\" + 0.011*\"espresso\" + 0.009*\"taste\" + 0.009*\"grinder\" + 0.008*\"brew\" + 0.008*\"drink\" + 0.007*\"ground\" + 0.007*\"best\" + 0.007*\"morning\" + 0.007*\"hot\" + 0.006*\"bean\" + 0.006*\"milk\" + 0.006*\"carafe\" + 0.005*\"strong\" + 0.005*\"thing\"\n",
      "\n",
      "Topic 17: 0.034*\"best\" + 0.020*\"opener\" + 0.020*\"ever\" + 0.016*\"say\" + 0.013*\"fan\" + 0.013*\"song\" + 0.010*\"eminem\" + 0.009*\"album\" + 0.006*\"beat\" + 0.006*\"retro\" + 0.006*\"open\" + 0.006*\"cd\" + 0.006*\"wine\" + 0.006*\"show\" + 0.006*\"really\" + 0.005*\"dont\" + 0.005*\"track\" + 0.005*\"america\" + 0.005*\"think\" + 0.005*\"people\"\n",
      "\n",
      "Topic 18: 0.055*\"oxo\" + 0.050*\"vacuum\" + 0.024*\"bag\" + 0.019*\"last\" + 0.019*\"long\" + 0.014*\"floor\" + 0.013*\"price\" + 0.012*\"brush\" + 0.010*\"light\" + 0.008*\"carpet\" + 0.008*\"old\" + 0.007*\"job\" + 0.007*\"weight\" + 0.007*\"im\" + 0.006*\"pick\" + 0.006*\"canister\" + 0.006*\"really\" + 0.006*\"too\" + 0.006*\"still\" + 0.005*\"mighty\"\n",
      "\n",
      "Topic 19: 0.023*\"perfect\" + 0.018*\"size\" + 0.015*\"small\" + 0.013*\"large\" + 0.010*\"food\" + 0.009*\"stainless\" + 0.009*\"also\" + 0.009*\"steel\" + 0.008*\"take\" + 0.008*\"cook\" + 0.008*\"kitchen\" + 0.007*\"grater\" + 0.007*\"space\" + 0.007*\"fit\" + 0.006*\"find\" + 0.006*\"come\" + 0.006*\"cheese\" + 0.006*\"grate\" + 0.006*\"enough\" + 0.006*\"other\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top 20 words in each learned topic\n",
    "for i in range(num_topics):\n",
    "    print('Topic %i: %s\\n' % (i, lda.print_topic(i, topn=20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be4c9d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import models \n",
    "# Save the pretrained model to the datapath\n",
    "dp = datapath(\"lda.model\")\n",
    "lda.save(dp)\n",
    "\n",
    "# Load the pretrained model from the datapath\n",
    "lda = models.ldamodel.LdaModel.load(dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e9506",
   "metadata": {},
   "source": [
    "## Exporting model using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42afbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "with open('C:/Users/Kedar/Desktop/AML/Amazon/amazon_clean.model','wb') as ama:\n",
    "    pickle.dump(lda,ama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f29178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
