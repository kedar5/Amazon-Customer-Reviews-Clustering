{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment analysis.ipynb ",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kedar5/Amazon-Customer-Reviews-Clustering/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#**Sentiment Analysis**\n",
        "\n",
        "**Author: Kedar Deshpande**\n",
        "\n",
        "\n",
        "###Sentiment Analysis for Marketing Analytics\n",
        "\n",
        "Sentiment Analysis for marketing analytics helps inspect the given text and identifies the prevailing \n",
        "emotional opinion within the text, by analyzing online comments and engagements and detremines the prevailing emotion as positive, negative or neutral.\n",
        "This helps marketeers and analysts understand customer sentiment at scale through data sources like youtube comments or product reviews.\n"
      ],
      "metadata": {
        "id": "cfGhQjPSQy5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you Begin\n",
        "###Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. You'll be using your google account credentials to authenticate yourself to GCP, so ensure your google account has atleast the following roles in the GCP Project created (https://cloud.google.com/iam/docs/understanding-roles) :\n",
        "  - ***roles/editor***\n",
        "  - ***roles/iam.securityAdmin***\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "6M-QyYqrjtgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install Pre-requisites\n",
        "Note: After this step your runtime will be restarted to apply the new packages"
      ],
      "metadata": {
        "id": "qzIq7gkUkXKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install a pip package in the current kernel and create directory\n",
        "print(\"Installing libraries\") \n",
        "!pip install --user --quiet google-cloud-language validators \n",
        "!pip install --user --quiet google-cloud-pipeline-components==0.1.7 kfp==1.8.2 \n",
        "!pip install --quiet google-cloud-aiplatform==1.4.3\n"
      ],
      "metadata": {
        "id": "wRXegHTJa85m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "# Automatically restart kernel after installs\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "hTfFX21g2qLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook Imports and Authenticating Google account\n",
        "You will be prompted to give colab access to your google account and drive\n",
        "\n",
        "\n",
        "In order to run this program you would need to give colab access to your google account id to authenticate you as a user in your gcp project, and google drive to temporarily store data."
      ],
      "metadata": {
        "id": "ywmVC85kt09k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from google.cloud import aiplatform as vertex\n",
        "from google_cloud_pipeline_components import \\\n",
        "    aiplatform as vertex_pipeline_components\n",
        "import kfp.dsl as dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import (\n",
        "    component as comp,\n",
        "    Input,\n",
        "    Output,\n",
        "    Dataset,\n",
        "    Metrics,\n",
        ")\n",
        "from google.colab import auth as google_auth\n",
        "\n",
        "google_auth.authenticate_user()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RBO3ycZYDJ55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Environment Variables\n",
        "Instructions:\n",
        "\n",
        " \n",
        "\n",
        "1. **PROJECT_ID**\n",
        "  - Enter the Project ID of the GCP Project created.\n",
        "\n",
        "2. **REGION**\n",
        " - Enter region to store the analysis results for GCS and Big Query.\n",
        " - For supported regions refer: https://cloud.google.com/bigquery/docs/locations#regions\n",
        "\n",
        "3. **BUCKET_ID**\n",
        "  - Enter the name of the bucket to be created.\n",
        "  - For naming convention, refer: https://cloud.google.com/storage/docs/naming-buckets\n",
        "\n",
        "4. **FILENAME**\n",
        " - Enter the filename to be used for the output in GCS Bucket and Big Query\n",
        "\n",
        "5. **DATATYPE**\n",
        " - Select Data type as 'Reviews' or 'Youtube' depending on the type of data you want to analyze.\n",
        "\n",
        "\n",
        "6. **DATASET_ID**\n",
        " - Enter name of Big Query Dataset to store the results.\n",
        " - For naming convention, refer: https://cloud.google.com/bigquery/docs/datasets#dataset-naming\n",
        "\n",
        "7. **TABLE_NAME**\n",
        " - Enter name of Big Query Table to store the results.\n",
        " - For naming convention, refer: https://cloud.google.com/bigquery/docs/tables#table_naming\n",
        "\n",
        "8. **YOUTUBE_DEVELOPER_KEY**\n",
        "  - To analyze ***YouTube*** Video Comment data, enable the Youtube API in the GCP Project created and create the YOUTUBE_DEVELOPER_KEY.\n",
        "    - To generate a YOUTUBE_DEVELOPER_KEY for Youtube API, refer the steps for API Keys : https://developers.google.com/youtube/registering_an_application\n",
        "\n",
        "  - To analyze ***Product Review*** data, leave the YOUTUBE_DEVLOPER_KEY blank.\n",
        "\n"
      ],
      "metadata": {
        "id": "OYjFfou6EX-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GCP Project ID of the project created\n",
        "PROJECT_ID = \"sentiment-analysis-dev\" # @param {type:\"string\"}\n",
        "\n",
        "# Region to store data in GCS Bucket and Big Query. \n",
        "# For supported regions refer: https://cloud.google.com/bigquery/docs/locations#regions\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Name of the bucket to be created (https://cloud.google.com/storage/docs/naming-buckets)\n",
        "BUCKET_ID = \"sentiment-analysis-data\" # @param {type:\"string\"}\n",
        "\n",
        "# Filename to be used for uploading to GCS and Big Query \n",
        "FILENAME = \"Product.csv\" # @param {type:\"string\"}\n",
        "\n",
        "# DataType Reviews or Youtube\n",
        "DATATYPE = \"Reviews\" #@param [\"Reviews\", \"Youtube\"]\n",
        "\n",
        "# Dataset ID to be created for Big Query (https://cloud.google.com/bigquery/docs/datasets#dataset-naming)\n",
        "DATASET_ID = \"SentimentAnalysis\"  # @param {type:\"string\"}\n",
        "\n",
        "# Table name to be created for Big Query (https://cloud.google.com/bigquery/docs/tables#table_naming) \n",
        "TABLE_NAME = \"Product_table\"  # @param {type:\"string\"}\n",
        "\n",
        "#Developer Key for the Youtube API\n",
        "YOUTUBE_DEVELOPER_KEY = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "PIPELINE_JSON_PKG_PATH = \"sentiment_analysis.json\"\n",
        "PIPELINE_ROOT = f\"gs://{BUCKET_ID}/pipeline_root\""
      ],
      "metadata": {
        "id": "qvoXYWXBDe1w"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Enable APIs"
      ],
      "metadata": {
        "id": "F3U_94amp3x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project $PROJECT_ID\n",
        "!gcloud config set ai/region $REGION\n",
        "!gcloud services enable aiplatform.googleapis.com\n",
        "!gcloud services enable compute.googleapis.com\n",
        "!gcloud services enable language.googleapis.com\n",
        "!gcloud services enable youtube.googleapis.com"
      ],
      "metadata": {
        "id": "CzIvBeYvGwEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Bucket"
      ],
      "metadata": {
        "id": "12PoUSbxcw0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "if not storage.Bucket(storage_client, BUCKET_ID).exists():\n",
        "  bucket = storage_client.create_bucket(BUCKET_ID,location = REGION)\n",
        "  print(\"Bucket {} created\".format(bucket.name))\n",
        "else:\n",
        "  print(\"Bucket {} exists\".format(BUCKET_ID))"
      ],
      "metadata": {
        "id": "EcmHQq_FcvKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Big Query Dataset"
      ],
      "metadata": {
        "id": "cdZN6d7m4m6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.exceptions import NotFound\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Construct a BigQuery client object.\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "dataset = bigquery.Dataset(PROJECT_ID + \".\"+ DATASET_ID)\n",
        "dataset.location = REGION\n",
        "\n",
        "try:\n",
        "    bq_client.get_dataset(PROJECT_ID + \".\"+ DATASET_ID)  # Make an API request.\n",
        "    print(\"Dataset {} already exists\".format(DATASET_ID))\n",
        "except NotFound:\n",
        "    print(\"Dataset {} does not exist\".format(DATASET_ID))\n",
        "    # Send the dataset to the API for creation, with an explicit timeout.\n",
        "    dataset = bq_client.create_dataset(dataset, timeout=90)  # Make an API request.\n",
        "    print(\"Created dataset {}.{}\".format(bq_client.project, dataset.dataset_id))\n"
      ],
      "metadata": {
        "id": "9JpQ6U_q4l2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update service-account IAM policy"
      ],
      "metadata": {
        "id": "ttmhZhIOHaM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update Necessary permissions on service account to be used by Kubeflow\n",
        "# Get Project number\n",
        "project_number = !gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\n",
        "# Define Compute Service Account\n",
        "ser_acc = \"serviceAccount:\"+project_number[0]+\"-compute@developer.gserviceaccount.com\"\n",
        "!gcloud projects add-iam-policy-binding  $PROJECT_ID --member=$ser_acc --role='roles/storage.objectAdmin'"
      ],
      "metadata": {
        "id": "KI79BOaR9OvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Input Youtube URLs/ Input Product Review Dataset\n",
        "Instructions:\n",
        "\n",
        "- *If* analyzing **Youtube** data, input comma separated video URLs in the youtube_video_urls parameter. And a comment_limit\n",
        "\n",
        "- If analyzing, **Product Review** data, a prompt will appear in colab below for you to upload a file in CSV format. (Note: Only schema requirement is presence of a 'Review' column)"
      ],
      "metadata": {
        "id": "fRgKn-4itzix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If Datatype youtube call the yt_main() function\n",
        "import validators\n",
        "from google.colab import files\n",
        "\n",
        "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
        "            \"\"\"Uploads blob to GCS bucket\n",
        "\n",
        "            Uploads blob to a GCS bucket, given source and destination\n",
        "\n",
        "            Args :\n",
        "            bucket_name :\n",
        "                Name of GCS bucket to upload blob to.\n",
        "            source_file_name :\n",
        "                souce file to be uploaded.\n",
        "            destination_blob_name :\n",
        "                destination to upload to blob\n",
        "            \"\"\"\n",
        "            try:\n",
        "                bucket = storage_client.bucket(bucket_name)\n",
        "                blob = bucket.blob(destination_blob_name)\n",
        "                blob.upload_from_filename(source_file_name)\n",
        "                \n",
        "            except Exception as error:\n",
        "                print(\"Exception is \",error) \n",
        "            print(\n",
        "                f\"\\n File {source_file_name} uploaded to gs://{bucket_name}/{destination_blob_name}.\"\n",
        "        )\n",
        "\n",
        "comment_limit = 3000 # @param{type:\"integer\"}\n",
        "youtube_video_urls = \"https://www.youtube.com/watch?v=VhozpHTzo14,https://www.youtube.com/watch?v=eUx75Pl0THU\"  # @param {type:\"string\"}\n",
        "# youtube_video_urls = ['https://www.youtube.com/watch?v=VhozpHTzo14','https://www.youtube.com/watch?v=WJJ4ORIGFXA',\n",
        "    #         'https://www.youtube.com/watch?v=eUx75Pl0THU','https://www.youtube.com/watch?v=cuBAZc7loSY','https://www.youtube.com/watch?v=R1hC5vnM4EA']\n",
        "if DATATYPE == 'Youtube':\n",
        "    uploaded_file = FILENAME\n",
        "    try:\n",
        "        urls = list(youtube_video_urls.split(\",\"))\n",
        "        for i in urls:\n",
        "            valid = validators.url(i)\n",
        "            if valid == True:\n",
        "                print(\"Url is valid\")\n",
        "                if \"www.youtube.com\" in i.split(\"https://\")[1]:\n",
        "                    print(\"Valid youtube url\")\n",
        "                else:\n",
        "                    raise ValueError(\"Url must belong to youtube\")\n",
        "            else:\n",
        "                raise ValueError(\"Invalid URL\")      \n",
        "    except Exception as error:\n",
        "        print(\"Error : \", error)\n",
        "# Else take input file containing Product review data\n",
        "else:\n",
        "    try:\n",
        "        print(\"Upload Product review Dataset in csv format\")\n",
        "        uploaded = files.upload()\n",
        "        uploaded_file = (list(uploaded.keys())[0]) \n",
        "        upload_blob(BUCKET_ID,'/content/'+uploaded_file,uploaded_file)\n",
        "    except Exception as error:\n",
        "        print(\"Exception is \",error)"
      ],
      "metadata": {
        "id": "daaW6IXsTiFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Components\n",
        "This section consists of the basic pipeline components being used"
      ],
      "metadata": {
        "id": "wo0DcTnf3gzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(1) create_yt_dataset\n",
        "\n",
        "This component creates the comment dataset from youtube video urls provided."
      ],
      "metadata": {
        "id": "WCpBgQcvDONH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@comp(base_image = \"python:3.9\",packages_to_install=[\"google-cloud-storage\",\"pandas\",\"google-api-python-client\",\"oauth2client==1.5.2\"])\n",
        "def create_yt_dataset(\n",
        "project_ID : str,\n",
        "developer_key : str,\n",
        "urls : str,\n",
        "comment_limit : int,\n",
        "bucket_ID : str,\n",
        "file_name : str\n",
        "):\n",
        "    #Code to test youtube API, retrieving comments by video ID\n",
        "\n",
        "    import os\n",
        "    import json\n",
        "    import glob\n",
        "    import shutil\n",
        "    import pandas as pd\n",
        "    from csv import writer\n",
        "    from google.cloud import storage\n",
        "    import googleapiclient.discovery\n",
        "    from urllib.parse import urlparse, parse_qs\n",
        "    \n",
        "\n",
        "\n",
        "    def gather_comments(youtube_client,vid_id):\n",
        "        \"\"\"Fetches comments from Youtube videos\n",
        "\n",
        "        Retrieves comment data for a given youtube video id\n",
        "\n",
        "        Args:\n",
        "        youtube_client :\n",
        "            Instance of the youtube client.\n",
        "        vid_id :\n",
        "            Video ID for the given youtube video.\n",
        "\n",
        "        Returns :\n",
        "        A dict mapping keys to the corresponding table row data\n",
        "        fetched. For example:\n",
        "\n",
        "        {'Comment' : 'I like this product', \n",
        "        'Author' : 'John Doe', \n",
        "        'Comment ID' : 'NDHH24JAH', \n",
        "        'Like Count' : '24', \n",
        "        'Video ID' : 'NSJAUJW232'\n",
        "        }\n",
        "        \"\"\"\n",
        "        # put comments extracted in specific lists for each column\n",
        "        comments, commentsId, likes, authors, videoId = [], [], [], [], []\n",
        "        try:\n",
        "            response = youtube_client.commentThreads().list(\n",
        "                    part = \"snippet,replies\",\n",
        "                    maxResults = 100,\n",
        "                    videoId = vid_id,\n",
        "                    textFormat = \"plainText\").execute()\n",
        "            page = 0\n",
        "            index = 0\n",
        "            while len(comments)<comment_limit:\n",
        "                page += 1\n",
        "                # for every comment in the response received\n",
        "                for item in response['items']:\n",
        "                    index += 1\n",
        "                    comment = item['snippet']['topLevelComment']\n",
        "                    author = comment['snippet']['authorDisplayName']\n",
        "                    text = comment['snippet']['textDisplay']\n",
        "                    comment_id = item['snippet']['topLevelComment']['id']\n",
        "                    like_count = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
        "                    # append the comment to the lists\n",
        "                    comments.append(text)\n",
        "                    authors.append(author)\n",
        "                    commentsId.append(comment_id)\n",
        "                    likes.append(like_count)\n",
        "                    videoId.append(vid_id)\n",
        "                    if item['snippet']['totalReplyCount'] >0:\n",
        "                        if 'replies' in item:\n",
        "                            # Check if replies actually prersent or deleted\n",
        "                            for reply in item['replies']['comments']:\n",
        "                                index += 1         \n",
        "                                # Extract reply\n",
        "                                text = reply['snippet']['textDisplay']\n",
        "                                author = reply['snippet']['authorDisplayName']\n",
        "                                comment_id = reply['id']\n",
        "                                like_count = reply['snippet']['likeCount']\n",
        "                                # append the comment to the lists\n",
        "                                comments.append(text)\n",
        "                                authors.append(author)\n",
        "                                commentsId.append(comment_id)\n",
        "                                likes.append(like_count)\n",
        "                                videoId.append(vid_id)\n",
        "                        # Call function to populate comment replies under thread\n",
        "                # get next page of comments\n",
        "                if 'nextPageToken' in response:\n",
        "                    response = youtube_client.commentThreads().list(\n",
        "                        part = \"snippet,replies\",\n",
        "                        maxResults = 100,\n",
        "                        videoId = vid_id,\n",
        "                        textFormat = \"plainText\",\n",
        "                        pageToken = response['nextPageToken']\n",
        "                    ).execute()\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            # return the comment data\n",
        "            print(\"Number of comments : \", index)\n",
        "            return dict({'Comment' : comments, 'Author' : authors, 'Comment ID' : commentsId, 'Like Count' : likes, 'Video ID' : videoId})\n",
        "        except Exception as error:\n",
        "            print(\"Exception in gather_comments() is \",error) \n",
        "\n",
        "    def get_video_title(youtube_client,vid_id):\n",
        "        \"\"\"Fetches Video title from Youtube videos\n",
        "\n",
        "        Retrieves video title for a given youtube video id\n",
        "\n",
        "        Args :\n",
        "        youtube_client :\n",
        "            Instance of the youtube client.\n",
        "        vid_id :\n",
        "            Video ID for the given youtube video.\n",
        "\n",
        "        Return :\n",
        "        A string containing the video title\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response_title = youtube_client.videos().list(\n",
        "                part = 'snippet',\n",
        "                id = vid_id\n",
        "            ).execute()\n",
        "            # get the video title\n",
        "            video_title = response_title['items'][0]['snippet']['title']\n",
        "            return video_title\n",
        "        except Exception as error:\n",
        "            print(\"Exception in get_video_title() is \",error) \n",
        "\n",
        "    def get_youtube():\n",
        "        \"\"\"Fetches Youtube client\n",
        "        Returns :\n",
        "        Instance of Youtube client\n",
        "        \"\"\"\n",
        "        # Currently disabled autholibs https verification \n",
        "        os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
        "        api_service_name = \"youtube\"\n",
        "        api_version = \"v3\"\n",
        "        # TODO: Take Developer_Key as input.\n",
        "        try: \n",
        "            youtube_client = googleapiclient.discovery.build(\n",
        "                api_service_name, api_version, developerKey = developer_key)\n",
        "            return youtube_client\n",
        "        except Exception as error:\n",
        "            print(\"Exception in get_youtube() is \", error)\n",
        "\n",
        "\n",
        "    def get_vid_id(url):\n",
        "        \"\"\"Fetches Video Id from Youtube urls\n",
        "\n",
        "        Retrieves video Id for a given youtube video url\n",
        "        Args :\n",
        "          url : url of the youtube video.\n",
        "        \n",
        "        Returns : Video id string\n",
        "        \"\"\"\n",
        "        # Returns the vid_id from the url eg: https://www.youtube.com/watch?v=WJJ4ORIGFXA the vid_id = WJJ4ORIGFXA\n",
        "        try:\n",
        "            parse_url = urlparse(url)\n",
        "            # Check if embed video\n",
        "            if 'embed' in parse_url.path:\n",
        "                term ='embed'\n",
        "            else:\n",
        "                term ='v'\n",
        "            vid_id_query = parse_qs(parse_url.query).get(term)\n",
        "            if vid_id_query:\n",
        "                return vid_id_query[0]\n",
        "            if parse_url.path.split('/'):\n",
        "                return parse_url.path.split('/')[-1]\n",
        "        except ValueError:\n",
        "            print(\"Invalid URL!\")\n",
        "\n",
        "\n",
        "    def save_as_csv(output_dict, filename):\n",
        "        # save dictionary as csv\n",
        "        output_df = pd.DataFrame(output_dict, columns = output_dict.keys())\n",
        "        output_df.to_csv(f'/content/{filename}.csv')\n",
        "\n",
        "\n",
        "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
        "        \"\"\"Uploads blob to GCS bucket\n",
        "\n",
        "        Uploads blob to a GCS bucket, given source and destination\n",
        "\n",
        "        Args :\n",
        "        bucket_name :\n",
        "            Name of GCS bucket to upload blob to.\n",
        "        source_file_name :\n",
        "            souce file to be uploaded.\n",
        "        destination_blob_name :\n",
        "            destination to upload to blob\n",
        "        \"\"\"\n",
        "        try:\n",
        "            storage_client = storage.Client(project=project_ID)\n",
        "            bucket = storage_client.bucket(bucket_name)\n",
        "            blob = bucket.blob(destination_blob_name)\n",
        "            blob.upload_from_filename(source_file_name)\n",
        "            print(\n",
        "                f\"\\n File {source_file_name} uploaded to gs://{bucket_name}/{destination_blob_name}.\"\n",
        "            )\n",
        "        except Exception as error:\n",
        "            print(\"Exception in upload_blob() is \",error) \n",
        "\n",
        "\n",
        "    def csv_combiner():\n",
        "        \"\"\"Combines CSV files generated for each video\n",
        "\n",
        "        Combines the csv files in case of multiple youtube urls entered\n",
        "        \"\"\"\n",
        "        #import csv files from folder\n",
        "        try:\n",
        "            path = r'/content/'\n",
        "            allFiles = glob.glob(path + \"/*.csv\")\n",
        "            allFiles.sort()  # orderinig files\n",
        "\n",
        "            with open('/content/final'+file_name, 'wb') as outfile:\n",
        "                for i, fname in enumerate(allFiles):\n",
        "                    with open(fname, 'rb') as infile:\n",
        "                        if i != 0:\n",
        "                            infile.readline()  # Throw away header on all but first file\n",
        "                        # Block copy rest of file from input to output without parsing\n",
        "                        shutil.copyfileobj(infile, outfile)\n",
        "                        print(fname + \" has been imported.\")\n",
        "        except Exception as error:\n",
        "            print(\"Exception is\", error)\n",
        "\n",
        "    def yt_main(urls):\n",
        "        \"\"\"Main function for when data_type is Youtube\n",
        "\n",
        "        Collates comment data for multiple Urls using Youtube API and creates a single csv file\n",
        "        \"\"\"\n",
        "        os.mkdir('/content')\n",
        "        urls = list(urls.split(\",\"))\n",
        "        # TODO: Take urls as user input.\n",
        "        for url in urls:\n",
        "            vid_id = get_vid_id(url) # get Video ID\n",
        "            youtube_client = get_youtube()  \n",
        "            title = get_video_title(youtube_client, vid_id)\n",
        "\n",
        "            print(\"Video : \" ,title)\n",
        "            rawList = gather_comments(youtube_client,vid_id)\n",
        "            if rawList != None:\n",
        "                # Save the csv file\n",
        "                save_as_csv(rawList,title)\n",
        "                upload_blob(bucket_ID,'/youtube/'+title,'youtube/'+title)\n",
        "\n",
        "        csv_combiner()\n",
        "        upload_blob(bucket_ID,'/content/final'+file_name,file_name)\n",
        "        uploaded_file = file_name\n",
        "\n",
        "    yt_main(urls)"
      ],
      "metadata": {
        "id": "rYpoWwljAG1Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2)sentiment_analysis\n",
        "This component identifies the positive and negative entities, calculates the word count, and sentiment score for the given input (reviews or comments)"
      ],
      "metadata": {
        "id": "8kcunficnqXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@comp(base_image=\"python:3.9\", packages_to_install=[\"nltk\",\" google-cloud-language\",\"pandas\",\"fsspec\",\"gcsfs\",\"google-cloud-bigquery\",\"google-cloud-storage\"] )\n",
        "def sentiment_analysis(\n",
        "    data_type : str,\n",
        "    file_name : str,\n",
        "    uploaded_file : str,\n",
        "    bucket_ID : str,\n",
        "    project_ID : str,\n",
        "    location : str,\n",
        "    dataset_ID : str,\n",
        "    table_name : str\n",
        "):\n",
        "    import os\n",
        "    import re\n",
        "    import nltk\n",
        "    import csv\n",
        "    import string\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from pathlib import Path  \n",
        "    from nltk.tokenize import sent_tokenize\n",
        "    from nltk.stem import WordNetLemmatizer \n",
        "    from nltk.corpus import stopwords # Import the stop word list\n",
        "    from nltk.tokenize import wordpunct_tokenize\n",
        "    from collections import Counter\n",
        "    from google.cloud.exceptions import NotFound\n",
        "    from google.cloud import storage\n",
        "    from google.cloud import bigquery\n",
        "    from operator import contains\n",
        "    from google.cloud import language_v1\n",
        "    # downloading nltk packages\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "    def analyze_comment_sentiment(postTexts):\n",
        "        \"\"\"Analyzes sentiment score for given sentences\n",
        "\n",
        "        Retrieves Sentiment score and magnitude for given text input using\n",
        "        Natural Language API's analyze_sentiment function\n",
        "\n",
        "        Args :\n",
        "        postTexts :\n",
        "            Sentences to be analyzed.\n",
        "        \n",
        "        Returns :\n",
        "        Pandas series containing sentiment score and magnitude\n",
        "        \"\"\"\n",
        "        type_=language_v1.types.Document.Type.PLAIN_TEXT\n",
        "        language = \"en\"\n",
        "        document = {\"content\": postTexts, \"type_\": type_, \"language\": language}\n",
        "        encodingType = language_v1.EncodingType.UTF8 # try with UTF32 as well\n",
        "        Sentiment = ''\n",
        "        positive_pool = []\n",
        "        negative_pool = []\n",
        "        all_entities = []\n",
        "        positive_entities = []\n",
        "        negative_entities = []\n",
        "        client = language_v1.LanguageServiceClient()\n",
        "\n",
        "        sentiment_response = client.analyze_sentiment(request = {'document': document, 'encoding_type': encodingType})\n",
        "\n",
        "        # Setting overall Sentiment from sentiment score and mangnitude\n",
        "        if sentiment_response.document_sentiment.score >= 0.1 and sentiment_response.document_sentiment.magnitude > 0.1: \n",
        "            Sentiment = 'Positive'\n",
        "        elif sentiment_response.document_sentiment.score < -0.1 and sentiment_response.document_sentiment.magnitude > 0.1:\n",
        "            Sentiment = 'Negative'\n",
        "        else :\n",
        "            Sentiment = 'Neutral' \n",
        "        # At a sentence level we find postive and negative sentences and pool them\n",
        "        for sentence in sentiment_response.sentences:\n",
        "            if sentence.sentiment.score >= 0.1 and sentence.sentiment.magnitude > 0.1:          \n",
        "                positive_pool.append(sentence.text.content)\n",
        "            elif sentence.sentiment.score < -0.1 and sentence.sentiment.magnitude > 0.1:\n",
        "                negative_pool.append(sentence.text.content)\n",
        "\n",
        "        all_entities_response = client.analyze_entities(request = {'document': document, 'encoding_type': encodingType})\n",
        "\n",
        "        positive_doc = {\"content\": ''.join(positive_pool), \"type_\": type_, \"language\": language}\n",
        "        negative_doc = {\"content\": ''.join(negative_pool), \"type_\": type_, \"language\": language}\n",
        "\n",
        "        positive_entities_response = client.analyze_entities(request ={'document': positive_doc, 'encoding_type': encodingType})\n",
        "        negative_entities_response = client.analyze_entities(request ={'document': negative_doc, 'encoding_type': encodingType})\n",
        "\n",
        "        for entity in all_entities_response.entities:\n",
        "            all_entities.append(entity.name)\n",
        "        for entity in positive_entities_response.entities:\n",
        "            positive_entities.append(entity.name)\n",
        "        for entity in negative_entities_response.entities:\n",
        "            negative_entities.append(entity.name)\n",
        "        return pd.Series([sentiment_response.document_sentiment.score,sentiment_response.document_sentiment.magnitude,Sentiment,\n",
        "                        positive_pool,negative_pool,all_entities,positive_entities,negative_entities])\n",
        "        \n",
        "\n",
        "\n",
        "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
        "        \"\"\"Uploads blob to GCS bucket\n",
        "\n",
        "        Uploads blob to a GCS bucket, given source and destination\n",
        "\n",
        "        Args:\n",
        "        bucket_name :\n",
        "            Name of GCS bucket to upload blob to.\n",
        "        source_file_name :\n",
        "            souce file to be uploaded.\n",
        "        destination_blob_name :\n",
        "            destination to upload to blob\n",
        "        \"\"\"\n",
        "        try:\n",
        "            storage_client = storage.Client(project=project_ID)\n",
        "            bucket = storage_client.bucket(bucket_name)\n",
        "            blob = bucket.blob(destination_blob_name)\n",
        "            blob.upload_from_filename(source_file_name)\n",
        "        except Exception as error:\n",
        "            print(\"Exception is \",error) \n",
        "        print(\n",
        "            f\"\\n File {source_file_name} uploaded to gs://{bucket_name}/{destination_blob_name}.\"\n",
        "        )\n",
        "\n",
        "\n",
        "    def analyze_video_level_sentiment(yt_data):\n",
        "        \"\"\"Analyzes overall sentiment score for the video as a whole.\n",
        "\n",
        "        Retrieves Sentiment score and magnitude for video by collating all \n",
        "        video comments and analyzing the overall sentiment using\n",
        "        Natural Language API's analyze_sentiment function. \n",
        "        The resulting csv file is uploaded to GCS\n",
        "        Args :\n",
        "        yt_data : Youtube dataset generated in previous component.\n",
        "        \"\"\"\n",
        "        try: \n",
        "            out_file = 'YTvideolevel.csv'\n",
        "            n_vid = yt_data['Video ID'].unique()\n",
        "            comment_list_by_vid = []\n",
        "            ytdict = dict.fromkeys(['Video ID','Comments'])\n",
        "            for i in range(0,len(n_vid)):\n",
        "                vid_data = yt_data.loc[yt_data['Video ID'] == n_vid[i]]\n",
        "                list_of_comments = [x for x in vid_data['Comment'].values.tolist()]\n",
        "                comment_list_by_vid.append(\" \".join(list_of_comments))\n",
        "            ytdict['Video ID'] = n_vid\n",
        "            ytdict['Comments'] = comment_list_by_vid\n",
        "            df = pd.DataFrame(ytdict)\n",
        "\n",
        "            df[['vid_polarity','vid_magnitude','Sentiment','positive_pool','negative_pool','all_entities','positive_entities','negative_entities']]= df['Comments'].apply(lambda x: analyze_comment_sentiment(x))\n",
        "            filepath = Path('/content/'+out_file)  \n",
        "            df.to_csv(filepath)\n",
        "            print(df.head(20))\n",
        "            # upload final dataset to GCS bucket\n",
        "            upload_blob(bucket_ID,filepath,out_file)\n",
        "            bq_job(project_ID, dataset_ID, 'YT_video_level_sentiment', bucket_ID, out_file)\n",
        "        except Exception as error:\n",
        "            print(\"Exception is \",error)\n",
        "\n",
        "    def bq_job(project_ID, dataset_ID, table_name, bucket_ID, file_name):\n",
        "        \"\"\"Creates Big Query Tables from GCS files.\n",
        "\n",
        "        Creates a Big Query table from the csvs in Cloud Storage\n",
        "        Args:\n",
        "          project_ID : Project ID\n",
        "          datasetID : BigQuery Dataset ID\n",
        "          table_name : name of table to be created\n",
        "          file_name : name of GCS source used to create table\n",
        "        \"\"\"\n",
        "        try:\n",
        "          \n",
        "          table_id = project_ID+\".\"+dataset_ID+\".\"+table_name\n",
        "\n",
        "          # Create Table Schema\n",
        "          job_config = bigquery.LoadJobConfig(\n",
        "              #skip_leading_rows=1,\n",
        "              source_format = bigquery.SourceFormat.CSV,\n",
        "              allow_quoted_newlines = True,\n",
        "              autodetect = True,\n",
        "          )\n",
        "          # CSV File Location (Cloud Storage Bucket)\n",
        "          uri = \"gs://\"+bucket_ID+\"/\"+file_name\n",
        "\n",
        "          # Create the Job\n",
        "          bq_client = bigquery.Client(project = project_ID, location =location)\n",
        "          try:\n",
        "            bq_client.get_table(table_id)  # check if table exists.\n",
        "            print(\"Table {} already exists.\".format(table_id))\n",
        "            bq_client.delete_table(table_id, not_found_ok=True)  # Delete table if exists\n",
        "            print(\"Deleted table '{}'.\".format(table_id))\n",
        "          except NotFound:\n",
        "            print(\"Table {} is not found.\".format(table_id))\n",
        "          csv_load_job = bq_client.load_table_from_uri(\n",
        "          uri, table_id, job_config=job_config\n",
        "          )\n",
        "          print(\"Created table '{}'.\".format(table_id))\n",
        "          csv_load_job.result()\n",
        "        except Exception as error:\n",
        "            print(\"Exception is \",error)\n",
        "\n",
        "    #Loop through Comment dataframe\n",
        "    def lemma(tokenized_words):\n",
        "        \"\"\"Generates lemmatized blocks of words from tokenized_words\n",
        "        Args: tokenized_words: tokenized words given as input\n",
        "        Returns: List of lemmatized words\n",
        "        \"\"\"\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        return [lemmatizer.lemmatize(w) for w in tokenized_words]\n",
        "\n",
        "    def freq_words(words,category):  \n",
        "        \"\"\"Determines the most frequent words\n",
        "\n",
        "        Retrieves the word counts for most frequent words for each comment/review\n",
        "        using the Natural language toolkit,\n",
        "        The resulting csv file is uploaded to GCS\n",
        "        Args: \n",
        "          words: Pool of positive or negative sentences\n",
        "          category: 'Positive' or 'Negative' category\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if data_type == 'Youtube':\n",
        "                # new_words are additional stop words added to be removed from lemmatizer blocks\n",
        "                new_words = (' ','I',\"'s\",'``',\"n't\",'...',\"'re\",\"'m\",\"'ve\",\"The\",\"''\",\"..\",\"....\",'wa','M','u',\"’\")\n",
        "            elif data_type == 'Reviews':\n",
        "                # new_words are additional stop words added to be removed from lemmatizer blocks\n",
        "                new_words = (' ','I',\"'s\",'``',\"n't\",'...',\"'re\",\"'m\",\"'ve\",\"The\",\"''\",\"..\",\"....\",'wa','M','u',\"’\",\"It\",'it',\"This\",\"'d\")\n",
        "            else :\n",
        "                raise ValueError(\"invalid DataType : \", data_type) \n",
        "\n",
        "            tokenized_words = nltk.word_tokenize(words)\n",
        "            word_list = []\n",
        "            tokenized_words = lemma(tokenized_words)\n",
        "            stop = stopwords.words('english')\n",
        "            # Adding additional stop words to remove from the lemmatizer blocks\n",
        "            for i in new_words:\n",
        "                stop.append(i)\n",
        "            # Creating word_list after removing punctuation and stop words \n",
        "            for i in tokenized_words :\n",
        "              if i not in stop and i not in string.punctuation:\n",
        "                word_list.append(i)\n",
        "\n",
        "            freqs = nltk.FreqDist(word_list)\n",
        "            final = pd.DataFrame(freqs.items(),columns = ('word', 'count'))\n",
        "\n",
        "            filepath = Path('/content/wordcount'+category+file_name)\n",
        "            final.to_csv(filepath) \n",
        "            # upload final dataset to GCS bucket\n",
        "            upload_blob(bucket_ID,filepath,'wordcount'+category+file_name)\n",
        "            bq_job(project_ID, dataset_ID, 'wordcount'+category, bucket_ID, 'wordcount'+category+file_name)\n",
        "        except Exception as error:\n",
        "            print(\"Exception is \", error)\n",
        "\n",
        "    def sentiment_main():\n",
        "        # main function that calculates sentiment, and uploads resulting csv to GCS\n",
        "        os.mkdir('/content')\n",
        "        #Check for data type\n",
        "        if data_type == 'Reviews':\n",
        "            data = pd.read_csv('gs://'+bucket_ID+'/'+uploaded_file)\n",
        "            data[['polarity','magnitude','Sentiment','positive_pool','negative_pool','all_entities','positive_entities','negative_entities']] = data['Review'].apply(lambda x: analyze_comment_sentiment(x)) # Utilize the analyze_comment_sentiment function\n",
        "            filepath = Path('/content/final'+file_name)\n",
        "        elif data_type == 'Youtube':\n",
        "            data = pd.read_csv('gs://'+bucket_ID+'/'+uploaded_file).iloc[: , 1:]\n",
        "            analyze_video_level_sentiment(data)\n",
        "            data[['polarity','magnitude','Sentiment','positive_pool','negative_pool','all_entities','positive_entities','negative_entities']] = data['Comment'].apply(lambda x: analyze_comment_sentiment(x)) # Utilize the analyze_comment_sentiment function\n",
        "            filepath = Path('/content/final'+file_name)\n",
        "        \n",
        "        # Get list of all positive and negative sentences\n",
        "        positive_sentences = data.positive_pool.sum()\n",
        "        negative_sentences = data.negative_pool.sum()\n",
        "        p_pool = ''.join(positive_sentences)\n",
        "        n_pool = ''.join(negative_sentences)\n",
        "        # Get positive and negative wordcounts\n",
        "        freq_words(p_pool,'Positive')\n",
        "        freq_words(n_pool,'Negative')\n",
        "\n",
        "        all_entities = data.explode('all_entities')\n",
        "\n",
        "        all_entities.to_csv(filepath) # save as csv\n",
        "        # upload final dataset to GCS bucket\n",
        "        upload_blob(bucket_ID,filepath,file_name)\n",
        "        # Create big query table from gcs\n",
        "        bq_job(project_ID, dataset_ID, table_name, bucket_ID, file_name)\n",
        "\n",
        "    sentiment_main() \n"
      ],
      "metadata": {
        "id": "EDsePKInoE2E"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute The Pipeline\n",
        "\n",
        "Now that the components are defined, we execute the sentiment_pipeline"
      ],
      "metadata": {
        "id": "e7qf2W1t4eFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define pipeline paramters\n",
        "pipeline_params = {\n",
        "    \"data_type\" : DATATYPE,\n",
        "    \"file_name\" : FILENAME,\n",
        "    \"uploaded_file\" : uploaded_file,\n",
        "    \"bucket_id\" : BUCKET_ID,\n",
        "    \"developer_key\" : YOUTUBE_DEVELOPER_KEY,\n",
        "    \"project_id\" : PROJECT_ID,\n",
        "    \"location\" : REGION,\n",
        "    \"dataset_id\" : DATASET_ID,\n",
        "    \"table_name\" : TABLE_NAME,\n",
        "    \"urls\" : youtube_video_urls,\n",
        "    \"comment_limit\" : int(comment_limit)\n",
        "}"
      ],
      "metadata": {
        "id": "FlYWBFCgSJ6W"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the sentiment pipeline\n",
        "\n",
        "@dsl.pipeline(\n",
        "\tname = \"sentimentanalysispipline\",\n",
        "\tdescription = \"A pipeline to perform Sentiment Analysis\"\n",
        ")\n",
        "def sentiment_pipeline(\n",
        "    data_type : str,\n",
        "    file_name : str,\n",
        "    uploaded_file : str,\n",
        "    bucket_id : str,\n",
        "    developer_key : str,\n",
        "    project_id : str,\n",
        "    location : str,\n",
        "    dataset_id : str,\n",
        "    table_name : str,\n",
        "    urls : str,\n",
        "    comment_limit : int\n",
        "):\n",
        "    with dsl.Condition(data_type == 'Reviews'):\n",
        "        sentiment_task = sentiment_analysis(data_type,file_name, uploaded_file,bucket_id,project_id,location,dataset_id,table_name)\n",
        "    with dsl.Condition(data_type == 'Youtube'):\n",
        "       yt_task = create_yt_dataset(project_id,developer_key,urls,comment_limit,bucket_id,file_name)\n",
        "       uploaded_file = file_name\n",
        "       sentiment_task = sentiment_analysis(data_type,file_name, uploaded_file,bucket_id,project_id,location,dataset_id,table_name).after(yt_task)\n"
      ],
      "metadata": {
        "id": "IWkAcx6ASlX1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling and runnning the pipeline\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func = sentiment_pipeline,\n",
        "    package_path = PIPELINE_JSON_PKG_PATH,\n",
        ")\n",
        "\n",
        "vertex.init(project = PROJECT_ID, location = REGION)\n",
        "\n",
        "pipeline_job = vertex.PipelineJob(\n",
        "    template_path = PIPELINE_JSON_PKG_PATH,\n",
        "    pipeline_root = PIPELINE_ROOT,\n",
        "    parameter_values = pipeline_params,\n",
        "    enable_caching = False,\n",
        "    display_name = \"sentimentanalysispipline\"\n",
        ")\n",
        "\n",
        "response = pipeline_job.run()\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afKDBsqwSw7P",
        "outputId": "d384b3d1-02f8-4d4e-bb7c-89daa2ea5dc3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/971809154392/locations/us-central1/pipelineJobs/sentimentanalysispipline-20220806001547\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/971809154392/locations/us-central1/pipelineJobs/sentimentanalysispipline-20220806001547')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/sentimentanalysispipline-20220806001547?project=971809154392\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/971809154392/locations/us-central1/pipelineJobs/sentimentanalysispipline-20220806001547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/971809154392/locations/us-central1/pipelineJobs/sentimentanalysispipline-20220806001547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/971809154392/locations/us-central1/pipelineJobs/sentimentanalysispipline-20220806001547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/971809154392/locations/us-central1/pipelineJobs/sentimentanalysispipline-20220806001547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/971809154392/locations/us-central1/pipelineJobs/sentimentanalysispipline-20220806001547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/971809154392/locations/us-central1/pipelineJobs/sentimentanalysispipline-20220806001547\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}