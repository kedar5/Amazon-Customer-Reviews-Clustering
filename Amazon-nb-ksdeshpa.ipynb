{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98873c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once to install the Gensim framework for topic modeling\n",
    "#!pip install gensim --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f81d8b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['overall' 'vote' 'verified' 'reviewTime' 'reviewerID' 'asin'\n",
      " 'reviewerName' 'reviewText' 'summary' 'unixReviewTime' 'style' 'image']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "true = True\n",
    "\n",
    "false = False\n",
    "def parse(path):\n",
    "    g=gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDataFrame(path):\n",
    "    i=0\n",
    "    df ={}\n",
    "    data = parse(path)\n",
    "    for d in data:\n",
    "        df[i] =d\n",
    "        i += 1\n",
    "        if i > 100000:\n",
    "            break\n",
    "    return pd.DataFrame.from_dict(df, orient ='index')\n",
    "\n",
    "df = getDataFrame(\"C:/Users/Kedar/Desktop/AML/Amazon/Home_and_Kitchen.json.gz\")\n",
    "\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb0a1f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>08 31, 2010</td>\n",
       "      <td>A3NSN9WOX8470M</td>\n",
       "      <td>0006564224</td>\n",
       "      <td>mmm</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "      <td>Many uses...</td>\n",
       "      <td>1283212800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>04 2, 2010</td>\n",
       "      <td>A2AMX0AJ2BUDNV</td>\n",
       "      <td>0006564224</td>\n",
       "      <td>John R. Welch</td>\n",
       "      <td>Seems a bit expensive for a plastic bottle, bu...</td>\n",
       "      <td>Dispenser bottle</td>\n",
       "      <td>1270166400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>11 5, 2015</td>\n",
       "      <td>A8LUWTIPU9CZB</td>\n",
       "      <td>0560467893</td>\n",
       "      <td>Linda Fahner</td>\n",
       "      <td>Great product, love it!!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1446681600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>10 29, 2015</td>\n",
       "      <td>AABKIIHAL0L66</td>\n",
       "      <td>0560467893</td>\n",
       "      <td>TheBlueChain</td>\n",
       "      <td>This is a sturdy floating corner shelf!  We mo...</td>\n",
       "      <td>Sturdy Shelf, Poor Installation Instructions</td>\n",
       "      <td>1446076800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>09 9, 2015</td>\n",
       "      <td>A3DA0KIQ5OBK5C</td>\n",
       "      <td>0560467893</td>\n",
       "      <td>angelaarden</td>\n",
       "      <td>I purchased 4 of these shelves. they look grea...</td>\n",
       "      <td>Look great - one bad one...</td>\n",
       "      <td>1441756800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall vote  verified   reviewTime      reviewerID        asin  \\\n",
       "0      5.0    2      True  08 31, 2010  A3NSN9WOX8470M  0006564224   \n",
       "1      5.0    2      True   04 2, 2010  A2AMX0AJ2BUDNV  0006564224   \n",
       "2      5.0  NaN      True   11 5, 2015   A8LUWTIPU9CZB  0560467893   \n",
       "3      4.0    4      True  10 29, 2015   AABKIIHAL0L66  0560467893   \n",
       "4      3.0  NaN      True   09 9, 2015  A3DA0KIQ5OBK5C  0560467893   \n",
       "\n",
       "    reviewerName                                         reviewText  \\\n",
       "0            mmm  I don't use these for their original use, and ...   \n",
       "1  John R. Welch  Seems a bit expensive for a plastic bottle, bu...   \n",
       "2   Linda Fahner                           Great product, love it!!   \n",
       "3   TheBlueChain  This is a sturdy floating corner shelf!  We mo...   \n",
       "4    angelaarden  I purchased 4 of these shelves. they look grea...   \n",
       "\n",
       "                                        summary  unixReviewTime style image  \n",
       "0                                  Many uses...      1283212800   NaN   NaN  \n",
       "1                              Dispenser bottle      1270166400   NaN   NaN  \n",
       "2                                    Five Stars      1446681600   NaN   NaN  \n",
       "3  Sturdy Shelf, Poor Installation Instructions      1446076800   NaN   NaN  \n",
       "4                   Look great - one bad one...      1441756800   NaN   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "corpus = df['reviewText'].values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b188583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image             0.008090\n",
       "vote              0.178718\n",
       "style             0.741553\n",
       "summary           0.999870\n",
       "reviewerName      0.999940\n",
       "overall           1.000000\n",
       "verified          1.000000\n",
       "reviewTime        1.000000\n",
       "reviewerID        1.000000\n",
       "asin              1.000000\n",
       "reviewText        1.000000\n",
       "unixReviewTime    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.notnull().mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c3ce678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reviewText.fillna(\"\",inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f88955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "# The WordNet Lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Acceptable parts of speech\n",
    "pos_map = {'N': wordnet.NOUN, 'J': wordnet.ADJ, 'V': wordnet.VERB, 'R': wordnet.ADV }\n",
    "# Function to reduce corpus to word lemmas, limited to nouns, verbs, etc. \n",
    "def clean_corpus(corpus):\n",
    "    cleaned = []\n",
    "    for text in corpus:\n",
    "        words = []\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            for (word, pos) in nltk.pos_tag(nltk.word_tokenize(sent)):\n",
    "                if not pos[0] in pos_map:\n",
    "                    continue\n",
    "                word = lemmatizer.lemmatize(word.lower(), pos_map[pos[0]])\n",
    "                words.append(word.lower())\n",
    "        cleaned.append(words)\n",
    "    return cleaned\n",
    "clean_texts = clean_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c16969a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be             181383\n",
      "have           76578\n",
      "use            48487\n",
      "do             46157\n",
      "not            34043\n",
      "n't            32868\n",
      "make           30402\n",
      "great          28618\n",
      "work           25027\n",
      "very           24820\n",
      "good           24270\n",
      "get            23404\n",
      "'s             21492\n",
      "so             21026\n",
      "well           20181\n",
      "buy            19578\n",
      "just           19520\n",
      "love           18929\n",
      "time           18300\n",
      "easy           18111\n",
      "product        15623\n",
      "up             15141\n",
      "year           14566\n",
      "more           12382\n",
      "out            11998\n",
      "clean          11693\n",
      "only           11271\n",
      "much           10293\n",
      "little         10188\n",
      "go             10127\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Print the 30 most frequent words, and add to stopword list\n",
    "counter = Counter([w for t in clean_texts for w in t])\n",
    "wordcounts = counter.most_common(n=30)\n",
    "stopwords = []\n",
    "for word, count in wordcounts:\n",
    "    stopwords.append(word)\n",
    "    print('%s%i' % (word.ljust(15), count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cde75386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from each lemmatized text block\n",
    "for i in range(len(clean_texts)):\n",
    "    j = 0\n",
    "    while j < len(clean_texts[i]):\n",
    "        if clean_texts[i][j] in stopwords:\n",
    "            del clean_texts[i][j]\n",
    "        else:\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6810a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['original', 'purchase', 'need', 'replacement', 'last', 'never', 'even', 'think', 'internet', 'find', 'new', 'one', 'pleasant', 'surprise', 'find', 'immediately', 'actually', 'pay', 'le']\n"
     ]
    }
   ],
   "source": [
    "# Show example text block after data preparation\n",
    "print(clean_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a3609cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "dictionary = Dictionary(clean_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in clean_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "661ed76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# Pretrain the model using the corpus and number of topics\n",
    "num_topics = 20\n",
    "lda = LdaMulticore(corpus, id2word=dictionary, num_topics=num_topics, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d013a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.45741329860959173\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=clean_texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ed9b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.065*\"*\" + 0.036*\"loaf\" + 0.021*\"cupcake\" + 0.017*\"say\" + 0.013*\"machine\" + 0.009*\"song\" + 0.008*\"scale\" + 0.007*\"show\" + 0.007*\"eminem\" + 0.007*\"album\" + 0.005*\"people\" + 0.005*\"hear\" + 0.005*\"cd\" + 0.004*\"really\" + 0.004*\"know\" + 0.004*\"bread\" + 0.004*\"track\" + 0.004*\"come\" + 0.004*\"think\" + 0.004*\"first\"\n",
      "\n",
      "Topic 1: 0.021*\"cake\" + 0.012*\"small\" + 0.008*\"too\" + 0.008*\"perfect\" + 0.008*\"need\" + 0.008*\"set\" + 0.008*\"kitchen\" + 0.008*\"thing\" + 0.007*\"also\" + 0.007*\"timer\" + 0.007*\"job\" + 0.007*\"size\" + 0.007*\"enough\" + 0.006*\"cookie\" + 0.006*\"room\" + 0.006*\"keep\" + 0.006*\"fan\" + 0.006*\"come\" + 0.005*\"really\" + 0.005*\"large\"\n",
      "\n",
      "Topic 2: 0.035*\"espresso\" + 0.027*\"canner\" + 0.026*\"nice\" + 0.022*\"recommend\" + 0.021*\"look\" + 0.017*\"quality\" + 0.015*\"price\" + 0.013*\"sturdy\" + 0.013*\"size\" + 0.012*\"highly\" + 0.012*\"sharpen\" + 0.011*\"fit\" + 0.011*\"exactly\" + 0.010*\"need\" + 0.009*\"ball\" + 0.008*\"small\" + 0.008*\"steel\" + 0.008*\"too\" + 0.007*\"want\" + 0.007*\"money\"\n",
      "\n",
      "Topic 3: 0.025*\"it\" + 0.024*\"nutmeg\" + 0.024*\"the\" + 0.016*\"le\" + 0.016*\"to\" + 0.015*\"creuset\" + 0.015*\"and\" + 0.015*\"this\" + 0.015*\"can\" + 0.014*\"a\" + 0.011*\"suction\" + 0.010*\"all\" + 0.009*\"in\" + 0.009*\"ok\" + 0.009*\"wa\" + 0.008*\".....\" + 0.008*\"for\" + 0.007*\"my\" + 0.006*\"is\" + 0.006*\"you\"\n",
      "\n",
      "Topic 4: 0.013*\"potato\" + 0.011*\"hold\" + 0.011*\"set\" + 0.010*\"jar\" + 0.010*\"spoon\" + 0.010*\"dishwasher\" + 0.010*\"large\" + 0.009*\"kitchen\" + 0.009*\">\" + 0.008*\"<\" + 0.008*\"small\" + 0.008*\"size\" + 0.007*\"hand\" + 0.007*\"too\" + 0.007*\"handle\" + 0.007*\"heavy\" + 0.007*\"look\" + 0.006*\"like\" + 0.006*\"steel\" + 0.006*\"also\"\n",
      "\n",
      "Topic 5: 0.038*\"vacuum\" + 0.018*\"braun\" + 0.010*\"advertised\" + 0.009*\"hose\" + 0.009*\"expectation\" + 0.009*\"want\" + 0.008*\"described\" + 0.008*\"attachment\" + 0.008*\"suction\" + 0.007*\"old\" + 0.007*\"floor\" + 0.007*\"bag\" + 0.007*\"tortilla\" + 0.006*\"hair\" + 0.006*\"powerful\" + 0.005*\"meet\" + 0.005*\"as\" + 0.005*\"mighty\" + 0.005*\"light\" + 0.005*\"canister\"\n",
      "\n",
      "Topic 6: 0.032*\"corn\" + 0.027*\"italy\" + 0.019*\"pancake\" + 0.008*\"http\" + 0.008*\"pop\" + 0.008*\"crazy\" + 0.008*\"<\" + 0.007*\"bound\" + 0.007*\"measuring\" + 0.007*\"amazing\" + 0.007*\"express\" + 0.006*\"eminem\" + 0.006*\"input\" + 0.006*\"beat\" + 0.006*\"way\" + 0.005*\"hidden\" + 0.005*\"class=\" + 0.005*\"american\" + 0.005*\"i\" + 0.004*\"ca\"\n",
      "\n",
      "Topic 7: 0.052*\"best\" + 0.032*\"ever\" + 0.023*\"coffee\" + 0.022*\"'ve\" + 0.017*\"opener\" + 0.015*\"grinder\" + 0.014*\"own\" + 0.014*\"grind\" + 0.011*\"can\" + 0.009*\"carafe\" + 0.009*\"open\" + 0.008*\"bottle\" + 0.007*\"far\" + 0.007*\"muffin\" + 0.006*\"now\" + 0.005*\"never\" + 0.005*\"thing\" + 0.005*\"canner\" + 0.005*\"find\" + 0.005*\"maker\"\n",
      "\n",
      "Topic 8: 0.018*\"toaster\" + 0.016*\"oven\" + 0.015*\"pan\" + 0.015*\"fry\" + 0.013*\"stick\" + 0.012*\"spatula\" + 0.012*\"grill\" + 0.011*\"cook\" + 0.010*\"egg\" + 0.010*\"thing\" + 0.008*\"come\" + 0.007*\"melt\" + 0.007*\"meat\" + 0.007*\"think\" + 0.007*\"food\" + 0.007*\"try\" + 0.006*\"butter\" + 0.006*\"too\" + 0.006*\"say\" + 0.006*\"nice\"\n",
      "\n",
      "Topic 9: 0.035*\"perfect\" + 0.033*\"gift\" + 0.022*\"price\" + 0.021*\"..\" + 0.017*\"excellent\" + 0.016*\"apple\" + 0.015*\"really\" + 0.015*\"awesome\" + 0.012*\"quality\" + 0.011*\"expect\" + 0.011*\"friend\" + 0.011*\"thank\" + 0.011*\"fast\" + 0.010*\"give\" + 0.009*\"pie\" + 0.009*\"christmas\" + 0.008*\"say\" + 0.008*\"item\" + 0.007*\"happy\" + 0.007*\"purchase\"\n",
      "\n",
      "Topic 10: 0.017*\"bowl\" + 0.016*\"water\" + 0.015*\"lid\" + 0.014*\"top\" + 0.013*\"kettle\" + 0.012*\"salad\" + 0.011*\"bottom\" + 0.010*\"spinner\" + 0.008*\"dry\" + 0.008*\"design\" + 0.007*\"handle\" + 0.007*\"keep\" + 0.007*\"plastic\" + 0.007*\"button\" + 0.006*\"down\" + 0.006*\"off\" + 0.006*\"spin\" + 0.006*\"put\" + 0.006*\"wash\" + 0.006*\"look\"\n",
      "\n",
      "Topic 11: 0.058*\"blade\" + 0.040*\"knife\" + 0.034*\"blender\" + 0.026*\"sharp\" + 0.015*\"processor\" + 0.013*\"cut\" + 0.010*\"disc\" + 0.009*\"cuisinart\" + 0.009*\"peeler\" + 0.009*\"grater\" + 0.009*\"fruit\" + 0.008*\"edge\" + 0.008*\"peel\" + 0.008*\"food\" + 0.008*\"grate\" + 0.008*\"cheese\" + 0.007*\"slice\" + 0.007*\"small\" + 0.007*\"job\" + 0.006*\"really\"\n",
      "\n",
      "Topic 12: 0.053*\"pressure\" + 0.050*\"cooker\" + 0.025*\"rice\" + 0.021*\"cook\" + 0.018*\"whisk\" + 0.013*\"pot\" + 0.009*\"cooking\" + 0.008*\"need\" + 0.006*\"aluminum\" + 0.006*\"minute\" + 0.006*\"food\" + 0.006*\"purchase\" + 0.006*\"come\" + 0.005*\"first\" + 0.005*\"now\" + 0.005*\"seal\" + 0.005*\"thing\" + 0.005*\"steam\" + 0.005*\"really\" + 0.005*\"gasket\"\n",
      "\n",
      "Topic 13: 0.048*\"blender\" + 0.025*\"smoothy\" + 0.024*\"cooky\" + 0.020*\"frozen\" + 0.019*\"fruit\" + 0.019*\"timer\" + 0.018*\"blend\" + 0.012*\"fine\" + 0.010*\"perfectly\" + 0.010*\"clock\" + 0.009*\"smoothie\" + 0.009*\"worth\" + 0.008*\"count\" + 0.008*\"glass\" + 0.007*\"penny\" + 0.007*\"pitcher\" + 0.007*\"describe\" + 0.006*\"real\" + 0.006*\"chain\" + 0.006*\"banana\"\n",
      "\n",
      "Topic 14: 0.037*\"i\" + 0.028*\"ice\" + 0.012*\"machine\" + 0.012*\"old\" + 0.011*\"last\" + 0.010*\"maker\" + 0.010*\"look\" + 0.010*\"bag\" + 0.010*\"long\" + 0.008*\"quality\" + 0.008*\"keep\" + 0.008*\"cream\" + 0.008*\"need\" + 0.007*\"really\" + 0.006*\"model\" + 0.006*\"pulp\" + 0.006*\"one\" + 0.006*\"small\" + 0.005*\"coffee\" + 0.005*\"also\"\n",
      "\n",
      "Topic 15: 0.053*\"pan\" + 0.019*\"cook\" + 0.012*\"food\" + 0.011*\"juice\" + 0.010*\"small\" + 0.009*\"oven\" + 0.009*\"juicer\" + 0.009*\"large\" + 0.009*\"garlic\" + 0.008*\"size\" + 0.008*\"perfect\" + 0.007*\"heavy\" + 0.007*\"orange\" + 0.006*\"other\" + 0.006*\"non-stick\" + 0.006*\"also\" + 0.006*\"iron\" + 0.006*\"'ve\" + 0.006*\"come\" + 0.006*\"dish\"\n",
      "\n",
      "Topic 16: 0.043*\"coffee\" + 0.031*\"cup\" + 0.019*\"tea\" + 0.015*\"water\" + 0.014*\"hot\" + 0.012*\"pot\" + 0.010*\"rice\" + 0.008*\"maker\" + 0.007*\"drink\" + 0.007*\"keep\" + 0.007*\"take\" + 0.007*\"waffle\" + 0.007*\"popper\" + 0.006*\"put\" + 0.006*\"taste\" + 0.005*\"try\" + 0.005*\"too\" + 0.005*\"lid\" + 0.005*\"'ve\" + 0.005*\"then\"\n",
      "\n",
      "Topic 17: 0.009*\"break\" + 0.009*\"month\" + 0.009*\"amazon\" + 0.009*\"purchase\" + 0.009*\"last\" + 0.008*\"new\" + 0.008*\"review\" + 0.008*\"first\" + 0.008*\"return\" + 0.007*\"blender\" + 0.007*\"now\" + 0.007*\"unit\" + 0.007*\"back\" + 0.006*\"order\" + 0.006*\"find\" + 0.006*\"problem\" + 0.006*\"week\" + 0.006*\"item\" + 0.006*\"day\" + 0.006*\"replacement\"\n",
      "\n",
      "Topic 18: 0.102*\"bread\" + 0.053*\"popcorn\" + 0.024*\"bake\" + 0.018*\"pop\" + 0.012*\"butter\" + 0.009*\"machine\" + 0.009*\"oil\" + 0.008*\"slice\" + 0.008*\"fresh\" + 0.007*\"recipe\" + 0.007*\"boiler\" + 0.007*\"taste\" + 0.007*\"thanks\" + 0.007*\"liner\" + 0.006*\"flour\" + 0.006*\"knife\" + 0.005*\"sandwich\" + 0.005*\"double\" + 0.005*\"add\" + 0.005*\"never\"\n",
      "\n",
      "Topic 19: 0.030*\"knife\" + 0.022*\"oxo\" + 0.014*\"cut\" + 0.013*\"handle\" + 0.009*\"quality\" + 0.009*\"'ve\" + 0.009*\"hand\" + 0.008*\"pizza\" + 0.008*\"tool\" + 0.007*\"kitchen\" + 0.007*\"other\" + 0.006*\"feel\" + 0.006*\"toast\" + 0.006*\"brush\" + 0.006*\"look\" + 0.005*\"cutter\" + 0.005*\"set\" + 0.005*\"like\" + 0.005*\"purchase\" + 0.005*\"board\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top 20 words in each learned topic\n",
    "for i in range(num_topics):\n",
    "    print('Topic %i: %s\\n' % (i, lda.print_topic(i, topn=20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2055508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import models \n",
    "# Save the pretrained model to the datapath\n",
    "dp = datapath(\"lda.model\")\n",
    "lda.save(dp)\n",
    "\n",
    "# Load the pretrained model from the datapath\n",
    "lda = models.ldamodel.LdaModel.load(dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801cd20",
   "metadata": {},
   "source": [
    "## Exporting model using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdbc7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "with open('C:/Users/Kedar/Desktop/AML/Amazon/ama.model','wb') as ama:\n",
    "    pickle.dump(lda,ama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ffc2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
